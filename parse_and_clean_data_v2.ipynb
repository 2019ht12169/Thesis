{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parse_and_clean_data-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2019ht12169/Thesis/blob/main/parse_and_clean_data_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qa9z0oQTdR"
      },
      "source": [
        "#import all required libraries\n",
        "!pip install better_profanity\n",
        "!pip install emoji\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from better_profanity import profanity\n",
        "import pandas as pd\n",
        "import csv\n",
        "import emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTyQF4W-Mwdo"
      },
      "source": [
        "# using google drive to upload dataset related files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QK4jMOC70RX"
      },
      "source": [
        "#from google.colab import files [ contingency to upload dataset files in case of drive connection failure]\n",
        "#upload=files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaW2oyxJcR_6"
      },
      "source": [
        "def is_custom_URL_pattern_matched(text):\n",
        "  return bool(text.find('http')!=-1) and bool(re.match(r\"https?:\\s*//\\S+|www\\.\\s*\\S+|[a-z]+\\.\\s*\\S+\", text))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cbAIy_0SCqo"
      },
      "source": [
        "# checking for pattern and http before removing url\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?:\\s*//\\S+|www\\.\\s*\\S+|[a-z]+\\.\\s*\\S+')\n",
        "    #url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    if is_custom_URL_pattern_matched(text):\n",
        "      url = url_pattern.sub(r'', text)\n",
        "    else:\n",
        "      url=text\n",
        "    return url"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT-lwXZKdCh7",
        "outputId": "42cc8019-6c1f-4535-edc9-9feb006276ac"
      },
      "source": [
        "print(remove_urls('http://www.amazon.com/gp/product/b00ppag8lk?'))\n",
        "print(remove_urls('Excellent. Excellent'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Excellent. Excellent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ztYRbJF3Nco"
      },
      "source": [
        "# replace all emoji's in dataset with equivalent text\n",
        "def replaceEmojis(text):\n",
        "  replacedText = emoji.demojize(text, delimiters=(' ',' '))\n",
        "  return replacedText"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVq0IrOW7fq0",
        "outputId": "8cb9d231-468b-4d94-ba1d-30d9f32132b4"
      },
      "source": [
        "print(replaceEmojis(\"hello hiüëç hiüòä\"))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello hi thumbs_up  hi smiling_face_with_smiling_eyes \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd6SJcalawvI"
      },
      "source": [
        "# removing single char reviews if any to avoid noise\n",
        "def is_single_char_sentence(text):\n",
        "  return len(text)==1 and bool(re.match(r\"\\w\", text))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBh9I03odz9z",
        "outputId": "c5efe8a0-1af0-4aa5-dc96-a0f82a37c2fe"
      },
      "source": [
        "print(is_single_char_sentence('a'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5LV2EXYSrye"
      },
      "source": [
        "# converting chat word terminology to full text\n",
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "ASAP=As Soon As Possible\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My Ass Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The Ass\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My Ass Off\n",
        "SK8=Skate\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "U=You\n",
        "U2=You Too\n",
        "WB=Welcome Back\n",
        "WTF=What The Fuck\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\"\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27BflFCfIpUv"
      },
      "source": [
        "#convert emoticons to equivalent text\n",
        "EMOTICONS = {\n",
        "    u\":‚Äë\\)\":\"Happy face\",\n",
        "    u\":\\)\":\"Happy face\",\n",
        "    u\":-\\]\":\"Happy face\",\n",
        "    u\":\\]\":\"Happy face\",\n",
        "    u\":-3\":\"Happy face\",\n",
        "    u\":3\":\"Happy face\",\n",
        "    u\":->\":\"Happy face\",\n",
        "    u\":>\":\"Happy face\",\n",
        "    u\"8-\\)\":\"Happy face\",\n",
        "    u\":o\\)\":\"Happy face\",\n",
        "    u\":-\\}\":\"Happy face\",\n",
        "    u\":\\}\":\"Happy face\",\n",
        "    u\":-\\)\":\"Happy face\",\n",
        "    u\":c\\)\":\"Happy face\",\n",
        "    u\":\\^\\)\":\"Happy face\",\n",
        "    u\"=\\]\":\"Happy face\",\n",
        "    u\"=\\)\":\"Happy face\",\n",
        "    u\":‚ÄëD\":\"Laughing\",\n",
        "    u\":D\":\"Laughing\",\n",
        "    u\"8‚ÄëD\":\"Laughing\",\n",
        "    u\"8D\":\"Laughing\",\n",
        "    u\"X‚ÄëD\":\"Laughing\",\n",
        "    u\"XD\":\"Laughing\",\n",
        "    u\"=D\":\"Laughing\",\n",
        "    u\"=3\":\"Laughing\",\n",
        "    u\"B\\^D\":\"Laughing\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‚Äë\\(\":\"angry\",\n",
        "    u\":-\\(\":\"angry\",\n",
        "    u\":\\(\":\"angry\",\n",
        "    u\":‚Äëc\":\"angry\",\n",
        "    u\":c\":\"angry\",\n",
        "    u\":‚Äë<\":\"angry\",\n",
        "    u\":<\":\"angry\",\n",
        "    u\":‚Äë\\[\":\"angry\",\n",
        "    u\":\\[\":\"angry\",\n",
        "    u\":-\\|\\|\":\"angry\",\n",
        "    u\">:\\[\":\"angry\",\n",
        "    u\":\\{\":\"angry\",\n",
        "    u\":@\":\"angry\",\n",
        "    u\">:\\(\":\"angry\",\n",
        "    u\":'‚Äë\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‚Äë\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‚Äë':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‚ÄëO\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‚Äëo\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‚Äë0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‚Äë\\)\":\"smirk\",\n",
        "    u\";\\)\":\"smirk\",\n",
        "    u\"\\*-\\)\":\"smirk\",\n",
        "    u\"\\*\\)\":\"smirk\",\n",
        "    u\";‚Äë\\]\":\"smirk\",\n",
        "    u\";\\]\":\"smirk\",\n",
        "    u\";\\^\\)\":\"smirk\",\n",
        "    u\":‚Äë,\":\"smirk\",\n",
        "    u\";D\":\"smirk\",\n",
        "    u\":‚ÄëP\":\"Tongue sticking out\",\n",
        "    u\":P\":\"Tongue sticking out\",\n",
        "    u\"X‚ÄëP\":\"Tongue sticking out\",\n",
        "    u\"XP\":\"Tongue sticking out\",\n",
        "    u\":‚Äë√û\":\"Tongue sticking out\",\n",
        "    u\":√û\":\"Tongue sticking out\",\n",
        "    u\":b\":\"Tongue sticking out\",\n",
        "    u\"d:\":\"Tongue sticking out\",\n",
        "    u\"=p\":\"Tongue sticking out\",\n",
        "    u\">:P\":\"Tongue sticking out\",\n",
        "    u\":‚Äë/\":\"Skeptical\",\n",
        "    u\":/\":\"Skeptical\",\n",
        "    u\":-[.]\":\"Skeptical\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical\",\n",
        "    u\">:/\":\"Skeptical\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical\",\n",
        "    u\"=/\":\"Skeptical\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical\",\n",
        "    u\":L\":\"Skeptical\",\n",
        "    u\"=L\":\"Skeptical\",\n",
        "    u\":S\":\"Skeptical\",\n",
        "    u\":‚Äë\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed\",\n",
        "    u\":‚Äëx\":\"Sealed lips\",\n",
        "    u\":x\":\"Sealed lips\",\n",
        "    u\":‚Äë#\":\"Sealed lips\",\n",
        "    u\":#\":\"Sealed lips\",\n",
        "    u\":‚Äë&\":\"Sealed lips\",\n",
        "    u\":&\":\"Sealed lips\",\n",
        "    u\"O:‚Äë\\)\":\"innocent\",\n",
        "    u\"O:\\)\":\"innocent\",\n",
        "    u\"0:‚Äë3\":\"innocent\",\n",
        "    u\"0:3\":\"innocent\",\n",
        "    u\"0:‚Äë\\)\":\"innocent\",\n",
        "    u\"0:\\)\":\"innocent\",\n",
        "    u\":‚Äëb\":\"Tongue sticking out\",\n",
        "    u\"0;\\^\\)\":\"innocent\",\n",
        "    u\">:‚Äë\\)\":\"devilish\",\n",
        "    u\">:\\)\":\"devilish\",\n",
        "    u\"\\}:‚Äë\\)\":\"devilish\",\n",
        "    u\"\\}:\\)\":\"devilish\",\n",
        "    u\"3:‚Äë\\)\":\"devilish\",\n",
        "    u\"3:\\)\":\"devilish\",\n",
        "    u\">;\\)\":\"devilish\",\n",
        "    u\"\\|;‚Äë\\)\":\"Cool\",\n",
        "    u\"\\|‚ÄëO\":\"Bored\",\n",
        "    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n",
        "    u\"#‚Äë\\)\":\"Party all night\",\n",
        "    u\"%‚Äë\\)\":\"confused\",\n",
        "    u\"%\\)\":\"confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‚Äë\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Embarrassed\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Embarrassed\",\n",
        "    u\"\\(-_-;\\)\":\"Embarrassed\",\n",
        "    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Embarrassed\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"sign of respect\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"sign of respect\",\n",
        "    u\"<\\(_ _\\)>\":\"sign of respect\",\n",
        "    u\"<m\\(__\\)m>\":\"sign of respect\",\n",
        "    u\"m\\(__\\)m\":\"sign of respect\",\n",
        "    u\"m\\(_ _\\)m\":\"sign of respect\",\n",
        "    u\"\\('_'\\)\":\"Crying\",\n",
        "    u\"\\(/_;\\)\":\"Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Crying\",\n",
        "    u\"\\(;_;\":\"Crying\",\n",
        "    u\"\\(;_:\\)\":\"Crying\",\n",
        "    u\"\\(;O;\\)\":\"Crying\",\n",
        "    u\"\\(:_;\\)\":\"Crying\",\n",
        "    u\"\\(ToT\\)\":\"Crying\",\n",
        "    u\";_;\":\"Crying\",\n",
        "    u\";-;\":\"Crying\",\n",
        "    u\";n;\":\"Crying\",\n",
        "    u\";;\":\"Crying\",\n",
        "    u\"Q\\.Q\":\"Crying\",\n",
        "    u\"T\\.T\":\"Crying\",\n",
        "    u\"QQ\":\"Crying\",\n",
        "    u\"Q_Q\":\"Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n",
        "    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling\",\n",
        "    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n",
        "    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‚ÄòA`\\)\":\"Deflated\"\n",
        "}\n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', ' '.join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "    return text"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Notg62XKCpBh",
        "outputId": "7913bc7b-b6ed-44ab-f40f-6bff9e56304d"
      },
      "source": [
        "# stop word removal [our use case is sentiment analysis so, keeping negation words intact]\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords_default = stopwords.words('english')\n",
        "stopwords_toBeRemoved=['no', 'nor', 'not','but','don',\"don't\",\"aren't\",'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "for word in stopwords_toBeRemoved:\n",
        "  stopwords_default.remove(word)\n",
        "STOPWORDS = set(stopwords_default)\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr7JhPCjxSir"
      },
      "source": [
        "# nullify profane words in dataset\n",
        "def filter_profanity(sentence): \n",
        "   profanity.load_censor_words()\n",
        "   if profanity.contains_profanity(sentence):\n",
        "    sentence = profanity.censor(sentence)\n",
        "   return sentence"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9xxyUnYl9bh"
      },
      "source": [
        "#various possible steps for preprocessing\n",
        "def preprocessText(reviewText):\n",
        "      # remove url's\n",
        "      reviewText = remove_urls(reviewText)\n",
        "      # remove html tags\n",
        "      soup = BeautifulSoup(reviewText)\n",
        "      reviewText = soup.get_text()\n",
        "      # convert acronyms/chat terminology to full text\n",
        "      reviewText = chat_words_conversion(reviewText)\n",
        "      #convert emoticons to text\n",
        "      reviewText = convert_emoticons(reviewText)\n",
        "      #convert emoji to text\n",
        "      reviewText = replaceEmojis(reviewText)\n",
        "      # replace these tags and quote,comma to avoid parsing conflicts\n",
        "      reviewText = reviewText.replace('\"','')\n",
        "      reviewText = reviewText.replace('<br>','')\n",
        "      reviewText = reviewText.replace('<br />','')\n",
        "      reviewText = reviewText.replace('/>','')\n",
        "      reviewText = re.sub(',', '', reviewText)\n",
        "      # remove stop words\n",
        "      #reviewText = remove_stopwords(reviewText)\n",
        "      # remove new line characters\n",
        "      #reviewText = reviewText.replace('\\n',' ')\n",
        "      # remove punctuation \n",
        "      #reviewText = re.sub(r'[^\\w\\s]', '', reviewText)\n",
        "      # do censoring\n",
        "      #reviewText = profanity.censor(reviewText)\n",
        "      #remove astricks to nullify censored words\n",
        "      #reviewText = reviewText.replace('*','')\n",
        "      # remove single char sentences\n",
        "      #reviewText = remove_single_char_sentences(reviewText)\n",
        "      # convert all text to lower case\n",
        "      reviewText = reviewText.lower()\n",
        "      return reviewText"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEp0_Fg6_3l9"
      },
      "source": [
        "#download data file from drive\n",
        "tsv_file='/content/drive/MyDrive/projects/data/amazon_reviews_Electronics.tsv'\n",
        "csv_table=pd.read_table(tsv_file,sep='\\t',error_bad_lines=False)\n",
        "# convert data file into csv format\n",
        "csv_table.to_csv('/content/drive/MyDrive/projects/data/reviews_Electronics.csv',index=False,header=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "Fp70lqOOOdA9",
        "outputId": "6a136ffb-6aca-4e3a-a349-7ceee76087a3"
      },
      "source": [
        "#making data frame out of the csv version of data file\n",
        "data = pd.read_csv('/content/drive/MyDrive/projects/data/reviews_Electronics.csv')\n",
        "# drop unneeded columns and print it\n",
        "df = data.drop(['marketplace','customer_id','review_id','product_id',\n",
        "       'product_parent','product_title', 'product_category','helpful_votes', 'total_votes', 'vine', 'verified_purchase'], axis='columns')\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>star_rating</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>As described.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>It works as advertising.</td>\n",
              "      <td>It works as advertising.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Works pissa</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>One Star</td>\n",
              "      <td>Did not work at all.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Overall pleased with the item</td>\n",
              "      <td>Works well. Bass is somewhat lacking but is pr...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   star_rating  ... review_date\n",
              "0            5  ...  2015-08-31\n",
              "1            5  ...  2015-08-31\n",
              "2            5  ...  2015-08-31\n",
              "3            1  ...  2015-08-31\n",
              "4            5  ...  2015-08-31\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY-8TNpKha1j"
      },
      "source": [
        "#create new attributes as per need \n",
        "#labeling data based on star rating and combining review headlines and body\n",
        "df.loc[df['star_rating'] < 4, 'label'] = '0' \n",
        "df.loc[df['star_rating'] >= 4, 'label'] = '1' \n",
        "df=df.assign(text=lambda x: x['review_headline']+' '+x['review_body'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZv0gejSDvjp"
      },
      "source": [
        "# creating fresh df with needed features\n",
        "import numpy as np\n",
        "df_map_result = pd.DataFrame({'label': df['label'],\n",
        "    'text': df['text'],\n",
        "    'review_date': df['review_date']})\n",
        "df_map_result = df_map_result.astype({'label':int,\n",
        "'text':str,\n",
        "'review_date':np.datetime64})\n",
        "\n",
        "df_map_result.to_csv(\"/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics.csv\",index=False,header=True)\n",
        "#df_map_result.sample(10001).to_csv(\"/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics_small.csv\",index=False,header=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQMCqcq1_NYJ"
      },
      "source": [
        "# utility method for writing headers to data set\n",
        "def writeHeadersToFile(file):\n",
        "  file.write('\"label\"')\n",
        "  file.write(\",\")\n",
        "  file.write('\"review_date\"')\n",
        "  file.write(\",\")\n",
        "  file.write('\"text\"')\n",
        "  file.write(\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5sewC6c2PSP"
      },
      "source": [
        "# utility method for writing data rows to data set\n",
        "def writeDataToFile(file,row):\n",
        "  file.write('\"{}\"'.format(row[0]))\n",
        "  file.write(\",\")\n",
        "  file.write('\"{}\"'.format(row[2]))\n",
        "  file.write(\",\")\n",
        "  file.write('\"{}\"'.format(preprocessText(row[1])))\n",
        "  file.write(\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENtNqBWNAtPA",
        "outputId": "7517faea-ae4f-4ef8-8d42-c49b23230268"
      },
      "source": [
        "# separating positive and negative reviews for understanding data better \n",
        "# and balancing train data set\n",
        "file1 = open(\"/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv\",\"w\")\n",
        "file2 = open(\"/content/drive/MyDrive/projects/data/dataFile_latest_neg.csv\",\"w\")\n",
        "writeHeadersToFile(file1)\n",
        "writeHeadersToFile(file2)\n",
        "poscount=0\n",
        "negcount=0\n",
        "with open('/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics.csv', 'r') as file:\n",
        "    reader = csv.reader(file, delimiter=',')\n",
        "    next(reader, None) \n",
        "    for row in reader:\n",
        "      if int(row[0]) == 0:\n",
        "        negcount = negcount+1\n",
        "        writeDataToFile(file2,row)\n",
        "      else:\n",
        "        poscount = poscount+1\n",
        "        writeDataToFile(file1,row)     \n",
        "print(negcount)\n",
        "print(poscount)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "775236\n",
            "2315788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKrIYLBbMWDu",
        "outputId": "8f5a80e8-04b8-4166-b898-ccc942b7a9cb"
      },
      "source": [
        "# balance higher class labels with lower class labels \n",
        "# by extracting subset of higher class\n",
        "actualPos = pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv')\n",
        "#actualPos.sample(negcount).to_csv(\"/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv\",index=False,header=True)\n",
        "actualPos.sample(775236).to_csv(\"/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv\",index=False,header=True)\n",
        "actualPos = pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv')\n",
        "actualPos.head()\n",
        "len(actualPos)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "775236"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaAWRlpytdlL"
      },
      "source": [
        "  # creating train and test dataset files after all preprocessing\n",
        "  file_1 = open(\"/content/drive/MyDrive/projects/data/dataFile_latest_test.csv\",\"w\")\n",
        "  file_2 = open(\"/content/drive/MyDrive/projects/data/dataFile_latest_train.csv\",\"w\")\n",
        "  writeHeadersToFile(file_1)\n",
        "  writeHeadersToFile(file_2)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkn6-R4UsA_R"
      },
      "source": [
        "#creating train and test datasets as combination of positive and negative reviews\n",
        "count = 0\n",
        "with open('/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv', 'r') as file:\n",
        "  reader = csv.reader(file, delimiter=',')\n",
        "  next(reader, None) \n",
        "  for row in reader:\n",
        "    count = count+1\n",
        "    if (count%5) == 0:\n",
        "      file_1.write('\"{}\"'.format(row[0]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[1]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[2]))\n",
        "      file_1.write(\"\\n\")\n",
        "    else:\n",
        "      file_2.write('\"{}\"'.format(row[0]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[1]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[2]))\n",
        "      file_2.write(\"\\n\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgm3D9IMt6IK"
      },
      "source": [
        "#creating train and test datasets as combination of positive and negative reviews\n",
        "count = 0\n",
        "with open('/content/drive/MyDrive/projects/data/dataFile_latest_neg.csv', 'r') as file:\n",
        "  reader = csv.reader(file, delimiter=',')\n",
        "  next(reader, None) \n",
        "  for row in reader:\n",
        "    count = count+1\n",
        "    if count%5 == 0:\n",
        "      file_1.write('\"{}\"'.format(row[0]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[1]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[2]))\n",
        "      file_1.write(\"\\n\")\n",
        "    else:\n",
        "      file_2.write('\"{}\"'.format(row[0]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[1]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[2]))\n",
        "      file_2.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}