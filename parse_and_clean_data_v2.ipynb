{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parse_and_clean_data-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2019ht12169/Thesis/blob/main/parse_and_clean_data_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qa9z0oQTdR"
      },
      "source": [
        "#import all required libraries\n",
        "!pip install better_profanity\n",
        "!pip install emoji\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from better_profanity import profanity\n",
        "import pandas as pd\n",
        "import csv\n",
        "import emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTyQF4W-Mwdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1438780-1d39-47de-bcaa-e089ee82ca58"
      },
      "source": [
        "# using google drive to upload dataset related files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QK4jMOC70RX"
      },
      "source": [
        "#from google.colab import files [ contingency to upload dataset files in case of drive connection failure]\n",
        "#upload=files.upload()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaW2oyxJcR_6"
      },
      "source": [
        "def is_custom_URL_pattern_matched(text):\n",
        "  return bool(text.find('http')!=-1) and bool(re.search(r\"https?:\\s*//\\S+|www\\.\\s*\\S+|[a-z]+\\.\\s*\\S+\", text))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cbAIy_0SCqo"
      },
      "source": [
        "# checking for pattern and http before removing url\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?:\\s*//\\S+|www\\.\\s*\\S+|[a-z]+\\.\\s*\\S+')\n",
        "    #url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    if is_custom_URL_pattern_matched(text):\n",
        "      url = url_pattern.sub(r'', text)\n",
        "    else:\n",
        "      url=text\n",
        "    return url"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6oQF8wDDOe1",
        "outputId": "f49a096d-aafb-4d14-c332-63c2cc3a6b43"
      },
      "source": [
        " #text=\"Capture comms and video while flying all day, using external power. I was using another version of this cable (comms only, no power). But on longer flights I found the GoPro battery just wasn't enough. I tried using one of those battery replacement cables but it kept cutting out (loose connection). I went back to using the internal battery, but augmenting that power with a 5000mAh battery (http://www.amazon.com/gp/product/B00MWU1GGI) with this power + comm cable. The combination is awesome, with a 64GB card (Hero 3+ Black), this cable, and that small external battery, I can fly 6+ hours capturing everything.\"\n",
        " text=\"Good quality, nice selection of sizes, just not enough color options. Good jumpers, just didn't have enough color choices.,they say the colors are random but I only received 4 colors and it wasn't enough for my needs. If you need more colors I would try these http://www.amazon.com/gp/product/B00JUKL4XI<br />I ordered these also and they come with 10 colors, male to maee, male to female, and female to female. 4 of each color and each story for 120 total. So for a dollar more of you need more colors they will work. If you need more of smaller and different size jumpers then these may work better for you since the others are all 8 inches long.\"\n",
        "print(remove_urls(text))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good quality, nice selection of sizes, just not enough color  jumpers, just didn't have enough color  say the colors are random but I only received 4 colors and it wasn't enough for my  you need more colors I would try these  />I ordered these also and they come with 10 colors, male to maee, male to female, and female to  of each color and each story for 120  for a dollar more of you need more colors they will  you need more of smaller and different size jumpers then these may work better for you since the others are all 8 inches long.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeadXH58C_oZ",
        "outputId": "d73fbfc6-f994-4437-bead-7c7bca3c45b5"
      },
      "source": [
        "\"Capture comms and video while flying all day, using external power. I was using another version of this cable (comms only, no power). But on longer flights I found the GoPro battery just wasn't enough. I tried using one of those battery replacement cables but it kept cutting out (loose connection). I went back to using the internal battery, but augmenting that power with a 5000mAh battery (http://www.amazon.com/gp/product/B00MWU1GGI) with this power + comm cable. The combination is awesome, with a 64GB card (Hero 3+ Black), this cable, and that small external battery, I can fly 6+ hours capturing everything.\".find('http')!=-1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TszhZipIC1ar",
        "outputId": "bd29a2ec-d1d4-436e-f002-2c11e983892c"
      },
      "source": [
        "is_custom_URL_pattern_matched(\"Capture comms and video while flying all day, using external power. I was using another version of this cable (comms only, no power). But on longer flights I found the GoPro battery just wasn't enough. I tried using one of those battery replacement cables but it kept cutting out (loose connection). I went back to using the internal battery, but augmenting that power with a 5000mAh battery (http://www.amazon.com/gp/product/B00MWU1GGI) with this power + comm cable. The combination is awesome, with a 64GB card (Hero 3+ Black), this cable, and that small external battery, I can fly 6+ hours capturing everything.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt4wahwJBhVT",
        "outputId": "faefff97-2483-42dc-fe91-3d1c4192e1a9"
      },
      "source": [
        "print(remove_urls(\"Capture comms and video while flying all day, using external power. I was using another version of this cable (comms only, no power). But on longer flights I found the GoPro battery just wasn't enough. I tried using one of those battery replacement cables but it kept cutting out (loose connection). I went back to using the internal battery, but augmenting that power with a 5000mAh battery (http://www.amazon.com/gp/product/B00MWU1GGI) with this power + comm cable. The combination is awesome, with a 64GB card (Hero 3+ Black), this cable, and that small external battery, I can fly 6+ hours capturing everything.\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Capture comms and video while flying all day, using external power. I was using another version of this cable (comms only, no power). But on longer flights I found the GoPro battery just wasn't enough. I tried using one of those battery replacement cables but it kept cutting out (loose connection). I went back to using the internal battery, but augmenting that power with a 5000mAh battery (http://www.amazon.com/gp/product/B00MWU1GGI) with this power + comm cable. The combination is awesome, with a 64GB card (Hero 3+ Black), this cable, and that small external battery, I can fly 6+ hours capturing everything.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT-lwXZKdCh7",
        "outputId": "39a41854-1f8b-4c88-f53a-157d11f4827c"
      },
      "source": [
        "print(remove_urls('http://www.amazon.com/gp/product/b00ppag8lk?'))\n",
        "print(remove_urls('Excellent. Excellent'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Excellent. Excellent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ztYRbJF3Nco"
      },
      "source": [
        "# replace all emoji's in dataset with equivalent text\n",
        "def replaceEmojis(text):\n",
        "  replacedText = emoji.demojize(text, delimiters=(' ',' '))\n",
        "  return replacedText"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVq0IrOW7fq0",
        "outputId": "869d048f-1f6a-4aa1-f424-25daf13a6465"
      },
      "source": [
        "print(replaceEmojis(\"hello hi👍 hi😊\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello hi thumbs_up  hi smiling_face_with_smiling_eyes \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd6SJcalawvI"
      },
      "source": [
        "# removing single char reviews if any to avoid noise\n",
        "def is_single_char_sentence(text):\n",
        "  return len(text)==1 and bool(re.match(r\"\\w\", text))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBh9I03odz9z",
        "outputId": "d6ca58e9-0dab-402b-ef14-b80a1a6d2dc8"
      },
      "source": [
        "print(is_single_char_sentence('a'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5LV2EXYSrye"
      },
      "source": [
        "# converting chat word terminology to full text\n",
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "ASAP=As Soon As Possible\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My Ass Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The Ass\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My Ass Off\n",
        "SK8=Skate\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "U=You\n",
        "U2=You Too\n",
        "WB=Welcome Back\n",
        "WTF=What The Fuck\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\"\n",
        "chat_words_map_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        cw = line.split(\"=\")[0]\n",
        "        cw_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(cw)\n",
        "        chat_words_map_dict[cw] = cw_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def chat_words_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27BflFCfIpUv"
      },
      "source": [
        "#convert emoticons to equivalent text\n",
        "EMOTICONS = {\n",
        "    u\":‑\\)\":\"Happy face\",\n",
        "    u\":\\)\":\"Happy face\",\n",
        "    u\":-\\]\":\"Happy face\",\n",
        "    u\":\\]\":\"Happy face\",\n",
        "    u\":-3\":\"Happy face\",\n",
        "    u\":3\":\"Happy face\",\n",
        "    u\":->\":\"Happy face\",\n",
        "    u\":>\":\"Happy face\",\n",
        "    u\"8-\\)\":\"Happy face\",\n",
        "    u\":o\\)\":\"Happy face\",\n",
        "    u\":-\\}\":\"Happy face\",\n",
        "    u\":\\}\":\"Happy face\",\n",
        "    u\":-\\)\":\"Happy face\",\n",
        "    u\":c\\)\":\"Happy face\",\n",
        "    u\":\\^\\)\":\"Happy face\",\n",
        "    u\"=\\]\":\"Happy face\",\n",
        "    u\"=\\)\":\"Happy face\",\n",
        "    u\":‑D\":\"Laughing\",\n",
        "    u\":D\":\"Laughing\",\n",
        "    u\"8‑D\":\"Laughing\",\n",
        "    u\"8D\":\"Laughing\",\n",
        "    u\"X‑D\":\"Laughing\",\n",
        "    u\"XD\":\"Laughing\",\n",
        "    u\"=D\":\"Laughing\",\n",
        "    u\"=3\":\"Laughing\",\n",
        "    u\"B\\^D\":\"Laughing\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‑\\(\":\"angry\",\n",
        "    u\":-\\(\":\"angry\",\n",
        "    u\":\\(\":\"angry\",\n",
        "    u\":‑c\":\"angry\",\n",
        "    u\":c\":\"angry\",\n",
        "    u\":‑<\":\"angry\",\n",
        "    u\":<\":\"angry\",\n",
        "    u\":‑\\[\":\"angry\",\n",
        "    u\":\\[\":\"angry\",\n",
        "    u\":-\\|\\|\":\"angry\",\n",
        "    u\">:\\[\":\"angry\",\n",
        "    u\":\\{\":\"angry\",\n",
        "    u\":@\":\"angry\",\n",
        "    u\">:\\(\":\"angry\",\n",
        "    u\":'‑\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‑\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‑':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‑O\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‑o\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‑0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‑\\)\":\"smirk\",\n",
        "    u\";\\)\":\"smirk\",\n",
        "    u\"\\*-\\)\":\"smirk\",\n",
        "    u\"\\*\\)\":\"smirk\",\n",
        "    u\";‑\\]\":\"smirk\",\n",
        "    u\";\\]\":\"smirk\",\n",
        "    u\";\\^\\)\":\"smirk\",\n",
        "    u\":‑,\":\"smirk\",\n",
        "    u\";D\":\"smirk\",\n",
        "    u\":‑P\":\"Tongue sticking out\",\n",
        "    u\":P\":\"Tongue sticking out\",\n",
        "    u\"X‑P\":\"Tongue sticking out\",\n",
        "    u\"XP\":\"Tongue sticking out\",\n",
        "    u\":‑Þ\":\"Tongue sticking out\",\n",
        "    u\":Þ\":\"Tongue sticking out\",\n",
        "    u\":b\":\"Tongue sticking out\",\n",
        "    u\"d:\":\"Tongue sticking out\",\n",
        "    u\"=p\":\"Tongue sticking out\",\n",
        "    u\">:P\":\"Tongue sticking out\",\n",
        "    u\":‑/\":\"Skeptical\",\n",
        "    u\":/\":\"Skeptical\",\n",
        "    u\":-[.]\":\"Skeptical\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical\",\n",
        "    u\">:/\":\"Skeptical\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical\",\n",
        "    u\"=/\":\"Skeptical\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical\",\n",
        "    u\":L\":\"Skeptical\",\n",
        "    u\"=L\":\"Skeptical\",\n",
        "    u\":S\":\"Skeptical\",\n",
        "    u\":‑\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed\",\n",
        "    u\":‑x\":\"Sealed lips\",\n",
        "    u\":x\":\"Sealed lips\",\n",
        "    u\":‑#\":\"Sealed lips\",\n",
        "    u\":#\":\"Sealed lips\",\n",
        "    u\":‑&\":\"Sealed lips\",\n",
        "    u\":&\":\"Sealed lips\",\n",
        "    u\"O:‑\\)\":\"innocent\",\n",
        "    u\"O:\\)\":\"innocent\",\n",
        "    u\"0:‑3\":\"innocent\",\n",
        "    u\"0:3\":\"innocent\",\n",
        "    u\"0:‑\\)\":\"innocent\",\n",
        "    u\"0:\\)\":\"innocent\",\n",
        "    u\":‑b\":\"Tongue sticking out\",\n",
        "    u\"0;\\^\\)\":\"innocent\",\n",
        "    u\">:‑\\)\":\"devilish\",\n",
        "    u\">:\\)\":\"devilish\",\n",
        "    u\"\\}:‑\\)\":\"devilish\",\n",
        "    u\"\\}:\\)\":\"devilish\",\n",
        "    u\"3:‑\\)\":\"devilish\",\n",
        "    u\"3:\\)\":\"devilish\",\n",
        "    u\">;\\)\":\"devilish\",\n",
        "    u\"\\|;‑\\)\":\"Cool\",\n",
        "    u\"\\|‑O\":\"Bored\",\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\n",
        "    u\"#‑\\)\":\"Party all night\",\n",
        "    u\"%‑\\)\":\"confused\",\n",
        "    u\"%\\)\":\"confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‑\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Embarrassed\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Embarrassed\",\n",
        "    u\"\\(-_-;\\)\":\"Embarrassed\",\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Embarrassed\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"sign of respect\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"sign of respect\",\n",
        "    u\"<\\(_ _\\)>\":\"sign of respect\",\n",
        "    u\"<m\\(__\\)m>\":\"sign of respect\",\n",
        "    u\"m\\(__\\)m\":\"sign of respect\",\n",
        "    u\"m\\(_ _\\)m\":\"sign of respect\",\n",
        "    u\"\\('_'\\)\":\"Crying\",\n",
        "    u\"\\(/_;\\)\":\"Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Crying\",\n",
        "    u\"\\(;_;\":\"Crying\",\n",
        "    u\"\\(;_:\\)\":\"Crying\",\n",
        "    u\"\\(;O;\\)\":\"Crying\",\n",
        "    u\"\\(:_;\\)\":\"Crying\",\n",
        "    u\"\\(ToT\\)\":\"Crying\",\n",
        "    u\";_;\":\"Crying\",\n",
        "    u\";-;\":\"Crying\",\n",
        "    u\";n;\":\"Crying\",\n",
        "    u\";;\":\"Crying\",\n",
        "    u\"Q\\.Q\":\"Crying\",\n",
        "    u\"T\\.T\":\"Crying\",\n",
        "    u\"QQ\":\"Crying\",\n",
        "    u\"Q_Q\":\"Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(一一\\)\":\"Shame\",\n",
        "    u\"\\(；一_一\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling\",\n",
        "    u\"\\(\\・\\・?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(ーー;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‘A`\\)\":\"Deflated\"\n",
        "}\n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', ' '.join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "    return text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Notg62XKCpBh",
        "outputId": "1ee34ec4-8a04-442d-ebbc-bb966c8602b1"
      },
      "source": [
        "# stop word removal [our use case is sentiment analysis so, keeping negation words intact]\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords_default = stopwords.words('english')\n",
        "stopwords_toBeRemoved=['no', 'nor', 'not','but','don',\"don't\",\"aren't\",'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "for word in stopwords_toBeRemoved:\n",
        "  stopwords_default.remove(word)\n",
        "STOPWORDS = set(stopwords_default)\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr7JhPCjxSir"
      },
      "source": [
        "# nullify profane words in dataset\n",
        "def filter_profanity(sentence): \n",
        "   profanity.load_censor_words()\n",
        "   if profanity.contains_profanity(sentence):\n",
        "    sentence = profanity.censor(sentence)\n",
        "   return sentence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9xxyUnYl9bh"
      },
      "source": [
        "#various possible steps for preprocessing\n",
        "def preprocessText(reviewText):\n",
        "      # remove url's\n",
        "      reviewText = remove_urls(reviewText)\n",
        "      # remove html tags\n",
        "      soup = BeautifulSoup(reviewText)\n",
        "      reviewText = soup.get_text()\n",
        "      # convert acronyms/chat terminology to full text\n",
        "      reviewText = chat_words_conversion(reviewText)\n",
        "      #convert emoticons to text\n",
        "      reviewText = convert_emoticons(reviewText)\n",
        "      #convert emoji to text\n",
        "      reviewText = replaceEmojis(reviewText)\n",
        "      # replace these tags and quote,comma to avoid parsing conflicts\n",
        "      reviewText = reviewText.replace('\"','')\n",
        "      reviewText = reviewText.replace('<br>','')\n",
        "      reviewText = reviewText.replace('<br />','')\n",
        "      reviewText = reviewText.replace('/>','')\n",
        "      reviewText = re.sub(',', '', reviewText)\n",
        "\n",
        "      #reviewText = reviewText.replace(\"\\[?\\[.+?\\]?\\]\", \" \")\n",
        "      #reviewText = reviewText.replace(\"\\/{3,}\", \" \")\n",
        "      #reviewText = reviewText.replace(\"\\&\\#.+\\&\\#\\d+?;\", \" \")\n",
        "      #reviewText = reviewText.replace(\"\\d+\\&\\#\\d+?;\", \" \")\n",
        "      #reviewText = reviewText.replace(\"\\&\\#\\d+?;\", \" \")\n",
        "\n",
        "      # remove stop words\n",
        "      #reviewText = remove_stopwords(reviewText)\n",
        "      # remove new line characters\n",
        "      #reviewText = reviewText.replace('\\n',' ')\n",
        "      # remove punctuation \n",
        "      #reviewText = re.sub(r'[^\\w\\s]', '', reviewText)\n",
        "      # do censoring\n",
        "      #reviewText = profanity.censor(reviewText)\n",
        "      #remove astricks to nullify censored words\n",
        "      #reviewText = reviewText.replace('*','')\n",
        "      # remove single char sentences\n",
        "      #reviewText = remove_single_char_sentences(reviewText)\n",
        "      # convert all text to lower case\n",
        "      reviewText = reviewText.lower()\n",
        "      return reviewText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEp0_Fg6_3l9"
      },
      "source": [
        "#download data file from drive\n",
        "tsv_file='/content/drive/MyDrive/projects/data/amazon_reviews_Electronics.tsv'\n",
        "csv_table=pd.read_table(tsv_file,sep='\\t',error_bad_lines=False)\n",
        "# convert data file into csv format\n",
        "csv_table.to_csv('/content/drive/MyDrive/projects/data/reviews_Electronics.csv',index=False,header=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Fp70lqOOOdA9",
        "outputId": "0b5e6855-f970-4a1f-d32d-2a105e717e18"
      },
      "source": [
        "#making data frame out of the csv version of data file\n",
        "data = pd.read_csv('/content/drive/MyDrive/projects/data/reviews_Electronics.csv')\n",
        "# drop unneeded columns and print it\n",
        "df = data.drop(['marketplace','customer_id','review_id','product_id',\n",
        "       'product_parent','product_title', 'product_category','helpful_votes', 'total_votes', 'vine', 'verified_purchase'], axis='columns')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>star_rating</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>As described.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>It works as advertising.</td>\n",
              "      <td>It works as advertising.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Works pissa</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>One Star</td>\n",
              "      <td>Did not work at all.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Overall pleased with the item</td>\n",
              "      <td>Works well. Bass is somewhat lacking but is pr...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   star_rating  ... review_date\n",
              "0            5  ...  2015-08-31\n",
              "1            5  ...  2015-08-31\n",
              "2            5  ...  2015-08-31\n",
              "3            1  ...  2015-08-31\n",
              "4            5  ...  2015-08-31\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY-8TNpKha1j"
      },
      "source": [
        "#create new attributes as per need \n",
        "#labeling data based on star rating and combining review headlines and body\n",
        "df.loc[df['star_rating'] < 4, 'label'] = '0' \n",
        "df.loc[df['star_rating'] >= 4, 'label'] = '1' \n",
        "df=df.assign(text=lambda x: x['review_headline']+' '+x['review_body'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZv0gejSDvjp"
      },
      "source": [
        "# creating fresh df with needed features\n",
        "import numpy as np\n",
        "df_map_result = pd.DataFrame({'label': df['label'],\n",
        "    'text': df['text'],\n",
        "    'review_date': df['review_date']})\n",
        "df_map_result = df_map_result.astype({'label':int,\n",
        "'text':str,\n",
        "'review_date':np.datetime64})\n",
        "\n",
        "df_map_result = df_map_result[df_map_result['text'].notna()]\n",
        "\n",
        "df_map_result.to_csv(\"/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics.csv\",index=False,header=True)\n",
        "#df_map_result.sample(10001).to_csv(\"/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics_small.csv\",index=False,header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Y_5PYH9RzN3N",
        "outputId": "7484a1e5-3cf1-4812-d73a-a86861db9bbb"
      },
      "source": [
        "df_map_result = pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics.csv',index_col=None)\n",
        "df_map_result.sample(200)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1942686</th>\n",
              "      <td>1</td>\n",
              "      <td>The best headset yet Bose has done it again an...</td>\n",
              "      <td>2013-06-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152806</th>\n",
              "      <td>1</td>\n",
              "      <td>Love this little case Love this little case.  ...</td>\n",
              "      <td>2015-07-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195141</th>\n",
              "      <td>1</td>\n",
              "      <td>Five Stars Great headphones at this price.</td>\n",
              "      <td>2015-07-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1640006</th>\n",
              "      <td>1</td>\n",
              "      <td>keep this at another location the extension is...</td>\n",
              "      <td>2014-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2898704</th>\n",
              "      <td>1</td>\n",
              "      <td>Great Radio for the Price!! I had no trouble a...</td>\n",
              "      <td>2008-07-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1953885</th>\n",
              "      <td>1</td>\n",
              "      <td>Works good. My antenna doesn't look quite like...</td>\n",
              "      <td>2013-06-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2946682</th>\n",
              "      <td>0</td>\n",
              "      <td>Somewhat disappointed I know of some Spanish-s...</td>\n",
              "      <td>2007-11-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976354</th>\n",
              "      <td>1</td>\n",
              "      <td>fine But it Don't work like I would like it .....</td>\n",
              "      <td>2014-11-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009514</th>\n",
              "      <td>1</td>\n",
              "      <td>Four Stars Works well with connections using s...</td>\n",
              "      <td>2014-10-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858124</th>\n",
              "      <td>1</td>\n",
              "      <td>Five Stars Does its job!</td>\n",
              "      <td>2014-12-18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                               text review_date\n",
              "1942686      1  The best headset yet Bose has done it again an...  2013-06-11\n",
              "152806       1  Love this little case Love this little case.  ...  2015-07-16\n",
              "195141       1         Five Stars Great headphones at this price.  2015-07-03\n",
              "1640006      1  keep this at another location the extension is...  2014-01-01\n",
              "2898704      1  Great Radio for the Price!! I had no trouble a...  2008-07-28\n",
              "...        ...                                                ...         ...\n",
              "1953885      1  Works good. My antenna doesn't look quite like...  2013-06-03\n",
              "2946682      0  Somewhat disappointed I know of some Spanish-s...  2007-11-29\n",
              "976354       1  fine But it Don't work like I would like it .....  2014-11-08\n",
              "1009514      1  Four Stars Works well with connections using s...  2014-10-25\n",
              "858124       1                           Five Stars Does its job!  2014-12-18\n",
              "\n",
              "[200 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQMCqcq1_NYJ"
      },
      "source": [
        "# utility method for writing headers to data set\n",
        "def writeHeadersToFile(file):\n",
        "  file.write('\"label\"')\n",
        "  file.write(\",\")\n",
        "  file.write('\"review_date\"')\n",
        "  file.write(\",\")\n",
        "  file.write('\"text\"')\n",
        "  file.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5sewC6c2PSP"
      },
      "source": [
        "# utility method for writing data rows to data set\n",
        "def writeDataToFile(file,row):\n",
        "  file.write('\"{}\"'.format(row[0]))\n",
        "  file.write(\",\")\n",
        "  file.write('\"{}\"'.format(row[2]))\n",
        "  file.write(\",\")\n",
        "  file.write('\"{}\"'.format(preprocessText(row[1])))\n",
        "  file.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENtNqBWNAtPA",
        "outputId": "f3bb285c-23b6-4430-b260-1894cb0a8d24"
      },
      "source": [
        "# separating positive and negative reviews for understanding data better \n",
        "# and balancing train data set\n",
        "file1 = open(\"/content/drive/MyDrive/projects/data/trail/dataFile_latest_pos.csv\",\"w\")\n",
        "file2 = open(\"/content/drive/MyDrive/projects/data/trail/dataFile_latest_neg.csv\",\"w\")\n",
        "writeHeadersToFile(file1)\n",
        "writeHeadersToFile(file2)\n",
        "poscount=0\n",
        "negcount=0\n",
        "with open('/content/drive/MyDrive/projects/data/dataFile_reviews_Electronics.csv', 'r') as file:\n",
        "    reader = csv.reader(file, delimiter=',')\n",
        "    next(reader, None) \n",
        "    for row in reader:\n",
        "      if int(row[0]) == 0:\n",
        "        negcount = negcount+1\n",
        "        writeDataToFile(file2,row)\n",
        "      else:\n",
        "        poscount = poscount+1\n",
        "        writeDataToFile(file1,row)     \n",
        "print(negcount)\n",
        "print(poscount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "775236\n",
            "2315788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKrIYLBbMWDu",
        "outputId": "8f5a80e8-04b8-4166-b898-ccc942b7a9cb"
      },
      "source": [
        "# balance higher class labels with lower class labels \n",
        "# by extracting subset of higher class\n",
        "actualPos = pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv')\n",
        "#actualPos.sample(negcount).to_csv(\"/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv\",index=False,header=True)\n",
        "actualPos.sample(775236).to_csv(\"/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv\",index=False,header=True)\n",
        "actualPos = pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv')\n",
        "actualPos.head()\n",
        "len(actualPos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "775236"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaAWRlpytdlL"
      },
      "source": [
        "  # creating train and test dataset files after all preprocessing\n",
        "  file_1 = open(\"/content/drive/MyDrive/projects/data/dataFile_latest_test.csv\",\"w\")\n",
        "  file_2 = open(\"/content/drive/MyDrive/projects/data/dataFile_latest_train.csv\",\"w\")\n",
        "  writeHeadersToFile(file_1)\n",
        "  writeHeadersToFile(file_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkn6-R4UsA_R"
      },
      "source": [
        "#creating train and test datasets as combination of positive and negative reviews\n",
        "count = 0\n",
        "with open('/content/drive/MyDrive/projects/data/dataFile_latest_pos.csv', 'r') as file:\n",
        "  reader = csv.reader(file, delimiter=',')\n",
        "  next(reader, None) \n",
        "  for row in reader:\n",
        "    count = count+1\n",
        "    if (count%5) == 0:\n",
        "      file_1.write('\"{}\"'.format(row[0]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[1]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[2]))\n",
        "      file_1.write(\"\\n\")\n",
        "    else:\n",
        "      file_2.write('\"{}\"'.format(row[0]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[1]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[2]))\n",
        "      file_2.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgm3D9IMt6IK"
      },
      "source": [
        "#creating train and test datasets as combination of positive and negative reviews\n",
        "count = 0\n",
        "with open('/content/drive/MyDrive/projects/data/dataFile_latest_neg.csv', 'r') as file:\n",
        "  reader = csv.reader(file, delimiter=',')\n",
        "  next(reader, None) \n",
        "  for row in reader:\n",
        "    count = count+1\n",
        "    if count%5 == 0:\n",
        "      file_1.write('\"{}\"'.format(row[0]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[1]))\n",
        "      file_1.write(\",\")\n",
        "      file_1.write('\"{}\"'.format(row[2]))\n",
        "      file_1.write(\"\\n\")\n",
        "    else:\n",
        "      file_2.write('\"{}\"'.format(row[0]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[1]))\n",
        "      file_2.write(\",\")\n",
        "      file_2.write('\"{}\"'.format(row[2]))\n",
        "      file_2.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}