{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AspectExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNhmpjF0JHV2rROU/ZePWIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2019ht12169/Thesis/blob/main/AspectExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjXkal3WUjKO"
      },
      "source": [
        "from sklearn import cluster\n",
        "from collections import defaultdict\n",
        "import pandas as pd"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSvqJoCloEnf"
      },
      "source": [
        "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "prod_pronouns = ['it','this','they','these']\n",
        "exclude_stopwords = ['it','this','they','these'] # update in aspect_extraction script as well to be replaced by 'product'"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEYPfC6AkPBv",
        "outputId": "4dcb55ed-42b2-40f8-df80-751c29cdf159"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpz5uSopDlpO"
      },
      "source": [
        "# using google drive to upload dataset related files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7L_lXJvGuBe"
      },
      "source": [
        "def fetch_data():\n",
        "  smallData =  pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_latest_test.csv',sep = \",\",index_col=None)\n",
        "  smallData.sample(1000).to_csv('/content/drive/MyDrive/projects/data/dataFile_latest_test_small.csv',sep = \",\",index=False,header=True)\n",
        "  return pd.read_csv('/content/drive/MyDrive/projects/data/dataFile_latest_test_small.csv',sep = \",\",index_col=None)"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDiIV2Gloa65"
      },
      "source": [
        "def init_spacy():\n",
        "     print(\"Loading Spacy\")\n",
        "     for w in stopwords:\n",
        "         nlp.vocab[w].is_stop = True\n",
        "     for w in exclude_stopwords:\n",
        "         nlp.vocab[w].is_stop = False\n",
        "     return nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQVlGtVndodw"
      },
      "source": [
        "def apply_extraction(row,nlp):\n",
        "    review_body = row['text']\n",
        "    #review_id = row['review_id']\n",
        "    # review_marketplace = row['marketplace']\n",
        "    # customer_id = row['customer_id']\n",
        "    #product_id = row['product_id']\n",
        "    # product_parent = row['product_parent']\n",
        "    # product_title = row['product_title']\n",
        "    # product_category = row['product_category']\n",
        "    # date = str(row['review_date'])\n",
        "    # star_rating = row['star_rating']\n",
        "    # url = add_amazonlink(product_id)\n",
        "\n",
        "\n",
        "\n",
        "    doc=nlp(review_body)\n",
        "    #print(\"--- SPACY : Doc loaded ---\")\n",
        "\n",
        "    ## FIRST RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    ## RULE = M is child of A with a relationshio of amod\n",
        "    rule1_pairs = []\n",
        "    rule2_pairs = []\n",
        "    rule3_pairs = []\n",
        "    rule4_pairs = []\n",
        "    rule5_pairs = []\n",
        "    rule6_pairs = []\n",
        "    rule7_pairs = []\n",
        "\n",
        "    for token in doc:\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        if token.dep_ == \"amod\" and not token.is_stop:\n",
        "            M = token.text\n",
        "            A = token.head.text\n",
        "\n",
        "            # add adverbial modifier of adjective (e.g. 'most comfortable headphones')\n",
        "            M_children = token.children\n",
        "            for child_m in M_children:\n",
        "                if(child_m.dep_ == \"advmod\"):\n",
        "                    M_hash = child_m.text\n",
        "                    M = M_hash + \" \" + M\n",
        "                    break\n",
        "\n",
        "            # negation in adjective, the \"no\" keyword is a 'det' of the noun (e.g. no interesting characters)\n",
        "            A_children = token.head.children\n",
        "            for child_a in A_children:\n",
        "                if(child_a.dep_ == \"det\" and child_a.text == 'no'):\n",
        "                    neg_prefix = 'not'\n",
        "                    M = neg_prefix + \" \" + M\n",
        "                    break\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict1 = {\"noun\" : A, \"adj\" : M}\n",
        "            rule1_pairs.append(dict1)\n",
        "\n",
        "        # print(\"--- SPACY : Rule 1 Done ---\")\n",
        "\n",
        "        # # SECOND RULE OF DEPENDANCY PARSE -\n",
        "        # # M - Sentiment modifier || A - Aspect\n",
        "        # Direct Object - A is a child of something with relationship of nsubj, while\n",
        "        # M is a child of the same something with relationship of dobj\n",
        "        # Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if((child.dep_ == \"dobj\" and child.pos_ == \"ADJ\") and not child.is_stop):\n",
        "                M = child.text\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict2 = {\"noun\" : A, \"adj\" : M}\n",
        "            rule2_pairs.append(dict2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"--- SPACY : Rule 2 Done ---\")\n",
        "\n",
        "        ## THIRD RULE OF DEPENDANCY PARSE -\n",
        "        ## M - Sentiment modifier || A - Aspect\n",
        "        ## Adjectival Complement - A is a child of something with relationship of nsubj, while\n",
        "        ## M is a child of the same something with relationship of acomp\n",
        "        ## Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "        ## \"The sound of the speakers would be better. The sound of the speakers could be better\" - handled using AUX dependency\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"acomp\" and not child.is_stop):\n",
        "                M = child.text\n",
        "\n",
        "            # example - 'this could have been better' -> (this, not better)\n",
        "            if(child.dep_ == \"aux\" and child.tag_ == \"MD\"):\n",
        "                neg_prefix = \"not\"\n",
        "                add_neg_pfx = True\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict3 = {\"noun\" : A, \"adj\" : M}\n",
        "            rule3_pairs.append(dict3)\n",
        "            #rule3_pairs.append((A, M, sid.polarity_scores(M)['compound'],3))\n",
        "    # print(\"--- SPACY : Rule 3 Done ---\")\n",
        "\n",
        "        ## FOURTH RULE OF DEPENDANCY PARSE -\n",
        "        ## M - Sentiment modifier || A - Aspect\n",
        "\n",
        "        #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n",
        "        # M is a child of the same something with relationship of advmod\n",
        "\n",
        "        #Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if((child.dep_ == \"nsubjpass\" or child.dep_ == \"nsubj\") and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"advmod\" and not child.is_stop):\n",
        "                M = child.text\n",
        "                M_children = child.children\n",
        "                for child_m in M_children:\n",
        "                    if(child_m.dep_ == \"advmod\"):\n",
        "                        M_hash = child_m.text\n",
        "                        M = M_hash + \" \" + child.text\n",
        "                        break\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict4 = {\"noun\" : A, \"adj\" : M}\n",
        "            rule4_pairs.append(dict4)\n",
        "            #rule4_pairs.append((A, M,sid.polarity_scores(M)['compound'],4)) # )\n",
        "\n",
        "    # print(\"--- SPACY : Rule 4 Done ---\")\n",
        "\n",
        "\n",
        "        ## FIFTH RULE OF DEPENDANCY PARSE -\n",
        "        ## M - Sentiment modifier || A - Aspect\n",
        "\n",
        "        #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n",
        "        # M has a child with relationship of cop\n",
        "\n",
        "        #Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        buf_var = \"999999\"\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"cop\" and not child.is_stop):\n",
        "                buf_var = child.text\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "        if(A != \"999999\" and buf_var != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict5 = {\"noun\" : A, \"adj\" : token.text}\n",
        "            rule5_pairs.append(dict5)\n",
        "\n",
        "    # print(\"--- SPACY : Rule 5 Done ---\")\n",
        "\n",
        "        ## SIXTH RULE OF DEPENDANCY PARSE -\n",
        "        ## M - Sentiment modifier || A - Aspect\n",
        "        ## Example - \"It ok\", \"ok\" is INTJ (interjections like bravo, great etc)\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        if(token.pos_ == \"INTJ\" and not token.is_stop):\n",
        "            for child in children :\n",
        "                if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                    A = child.text\n",
        "                    M = token.text\n",
        "                    # check_spelling(child.text)\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict6 = {\"noun\" : A, \"adj\" : M}\n",
        "            rule6_pairs.append(dict6)\n",
        "\n",
        "\n",
        "    # print(\"--- SPACY : Rule 6 Done ---\")\n",
        "\n",
        "    ## SEVENTH RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    ## ATTR - link between a verb like 'be/seem/appear' and its complement\n",
        "    ## Example: 'this is garbage' -> (this, garbage)\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if((child.dep_ == \"attr\") and not child.is_stop):\n",
        "                M = child.text\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            if A in prod_pronouns :\n",
        "                A = \"product\"\n",
        "            dict7 = {\"noun\" : A, \"adj\" : M}\n",
        "            rule7_pairs.append(dict7)\n",
        "            #rule7_pairs.append((A, M,sid.polarity_scores(M)['compound'],7))\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"--- SPACY : Rules Done ---\")\n",
        "\n",
        "\n",
        "    aspects = []\n",
        "\n",
        "    aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs + rule6_pairs + rule7_pairs\n",
        "    dic = { \"aspect_pairs\" : aspects}\n",
        "    return dic\n"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBXYfuK9qsQC"
      },
      "source": [
        "def get_aspects(reviews_data):\n",
        "    aspects = []\n",
        "    for review in reviews_data:\n",
        "        aspect_pairs = review['aspect_pairs']\n",
        "        for map in aspect_pairs:\n",
        "            aspects.append(map['noun'])\n",
        "    return aspects"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cztMoO719rNq"
      },
      "source": [
        "def get_aspect_freq_map(aspects):\n",
        "    aspect_freq_map = defaultdict(int)\n",
        "    for asp in aspects:\n",
        "        aspect_freq_map[asp] += 1\n",
        "    return aspect_freq_map"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ1SGfKB_Zcm"
      },
      "source": [
        "def add_clusters_to_reviews(reviews_data,nlp):\n",
        "    product_aspects = get_aspects(reviews_data)\n",
        "    aspect_freq_map = get_aspect_freq_map(product_aspects)\n",
        "    unique_aspects = aspect_freq_map.keys()\n",
        "\n",
        "    aspect_labels = get_word_clusters(unique_aspects, nlp)\n",
        "    asp_to_cluster_map = dict(zip(unique_aspects, aspect_labels))\n",
        "    cluster_names_map = get_cluster_names_map(asp_to_cluster_map, aspect_freq_map)\n",
        "    updated_reviews = []\n",
        "\n",
        "    for review in reviews_data:\n",
        "        aspect_pairs_upd = []\n",
        "        aspect_pairs = review[\"aspect_pairs\"]\n",
        "        for map in aspect_pairs:\n",
        "            noun = map['noun']\n",
        "            cluster_label_id = asp_to_cluster_map[noun]\n",
        "            cluster_label_name = cluster_names_map[cluster_label_id]\n",
        "            map['cluster'] = cluster_label_name\n",
        "            aspect_pairs_upd.append(map)\n",
        "\n",
        "        review['aspect_pairs'] = aspect_pairs_upd\n",
        "        updated_reviews.append(review)\n",
        "    result = updated_reviews\n",
        "    return result"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSmD40QA_nt-"
      },
      "source": [
        "def get_word_vectors(unique_aspects, nlp):\n",
        "    asp_vectors = []\n",
        "    for aspect in unique_aspects:\n",
        "        token = nlp(aspect)\n",
        "        asp_vectors.append(token.vector)\n",
        "    return asp_vectors\n",
        "\n",
        "def get_word_clusters(unique_aspects, nlp):\n",
        "    # print(\"Found {} unique aspects for this product\".format(len(unique_aspects)))\n",
        "    asp_vectors = get_word_vectors(unique_aspects, nlp)\n",
        "    if len(unique_aspects) <= NUM_CLUSTERS:\n",
        "        # print(\"Too few aspects ({}) found. No clustering required...\".format(len(unique_aspects)))\n",
        "        return list(range(len(unique_aspects)))\n",
        "\n",
        "    # print(\"Running k-means clustering...\")\n",
        "    n_clusters = NUM_CLUSTERS\n",
        "    kmeans = cluster.KMeans(n_clusters=n_clusters)\n",
        "    kmeans.fit(asp_vectors)\n",
        "    labels = kmeans.labels_\n",
        "    return labels\n",
        "\n",
        "def get_cluster_names_map(asp_to_cluster_map, aspect_freq_map):\n",
        "    cluster_id_to_name_map = defaultdict()\n",
        "    clusters = set(asp_to_cluster_map.values())\n",
        "    for i in clusters:\n",
        "        this_cluster_asp = [k for k,v in asp_to_cluster_map.items() if v == i]\n",
        "        filt_freq_map = {k:v for k,v in aspect_freq_map.items() if k in this_cluster_asp}\n",
        "        filt_freq_map = sorted(filt_freq_map.items(), key = lambda x: x[1], reverse = True)\n",
        "        cluster_id_to_name_map[i] = filt_freq_map[0][0]\n",
        "    return cluster_id_to_name_map"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w73RXIKZsrwf"
      },
      "source": [
        "def extract_aspects(reviews,nlp):\n",
        "\n",
        "    #reviews = df[['review_id', 'review_body']]\n",
        "    # nlp = init_spacy()\n",
        "    # sid = init_nltk()\n",
        "\n",
        "    aspect_list = reviews.apply(lambda row: apply_extraction(row,nlp), axis=1) #going through all the rows in the dataframe\n",
        "    return aspect_list\n"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCWddPtyp25n"
      },
      "source": [
        "reviews_data = fetch_data()\n",
        "aspect_list = extract_aspects(reviews_data,nlp)\n",
        "updated_reviews = add_clusters_to_reviews(aspect_list, nlp)\n",
        "reviews_data['aspect_list']=updated_reviews\n",
        "reviews_data.to_csv('/content/drive/MyDrive/projects/data/dataFile_latest_test_results.csv',index=False)"
      ],
      "execution_count": 248,
      "outputs": []
    }
  ]
}